---
title: 'Step 2: Query MongoDB, transform data, and write to PostreSQL DB'
author: "Neil Kester"
date: "4/24/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      messages = FALSE)
```

# Purpose

These functions execute the extract, transform, and load (ETL) steps taking the data required to support analysis from the simulation's MongoDB logs, structuring and relating it, and then loading it into a relational database (in this case PostgreSQL) in preparation for later analysis.  

# Utility Function

These functions are used throughout the step and are considered "Utilities" because they augment workflows in a consistent manner.  

## Create a PostgreSQL or SQLite INSERT Query Based on a Provided Tibble of Data: `fillTableQuery()`  

This is a function I wrote in 2019 to convert an R tibble into a `SQLite` or `PostgreSQL` `INSERT` query. It is fairly robust. One limitation is the size of the resulting query string. Deal with this by using `batch_fillAndWrite()`.

```{r}
#' SQL Insert Query Builder
#'
#' This function takes a data frame and a table name (optionally including the
#'   field names in parentheses) and returns a string of a properly built SQL
#'   INSERT Query. This is most used with SQLite and PostgreSQL. Of note, to
#'   add the proper entry for auto-incrementing columns, pass an `NA` value for
#'   that field in the `data` parameter and provide the keyword in the `serial`
#'   parameter.
#'
#' @author Neil Kester, \email{nkester1@@jhu.edu}
#'
#' @param data This is a tibble of any dimension although thought should be given
#'  to very large data sets. It may be better to break the INSERT query into multiple
#'  smaller pushes.
#' @param tableName This is the name of the table. Note that some DBMS (namely)
#'   PostgreSQL does not automatically honor case. It is best to be explicit by
#'   wrapping all names within \"<name>\". Additionally, you may be explicity by
#'   including in this parameter the order of the table's fields you are providing.
#'   This ensures the columns provided in the `data` parameter go to the proper
#'   location. Do this like this: tableName = "\"<tableName\" (\"fieldOne\",
#'   \"fieldTwo\")".
#' @param serial Any field in the `data` parameter with `NA` will be treated as
#'   if it is the auto-incrementing primary key for the table. If using
#'   PostgreSQL this should be "DEFAULT". If using SQLite it should be "NULL".
#'
#' @return This returns a character string that can be passed to a DBI::dbSendQuery
#'   function for execution on the DBMS.
#'
#' @export fillTableQuery
#'
#' @note Location: ./R/fct_utils_fillTableQuery.R
#' @note RMarkdown location: ./inst/step2_queryMongoAndFillPg/Step2_queryMongoAndFillPg.Rmd
fillTableQuery <- function(data,
                           tableName,
                           serial = "DEFAULT"){

  query<-sprintf("INSERT INTO %s VALUES",
                 tableName)

  for(rdx in 1:nrow(data)){

    query <- paste(query,
                   "(",
                   sep = '')

    row <- NULL

    for(cdx in 1:ncol(data)){

      if(cdx == ncol(data)){

        if(is.na(data[rdx,cdx][[1]])){

          row <- paste(row, 
                       serial, 
                       sep = '')

        }else{

          row <- paste(row,
                       "'",
                       data[rdx,cdx][[1]],
                       "'",
                       sep = '')

        } # close else

      }else{

        if(is.na(data[rdx,cdx][[1]])){

          row <-paste(row,
                      serial,
                      ",",
                      sep = '')

        }else{

          row <- paste(row,
                       "'",
                       data[rdx,cdx][[1]],
                       "',",
                       sep = '')

        } # close else
        
      } # close else

    }#close cdx loop

    query <- paste(query,
                   row,
                   sep = '')

    if(rdx == nrow(data)){

      query <- paste(query,
                     ")",
                     sep = '')

    }else{#if complete

      query <- paste(query,
                     "),",
                     sep = '')

    }#close else

  }#close rdx loop

  return(query)

}#close fillTableQuery function

```


## Call the `fillTableQuery()` in batches: `batch_fillAndWrite()`  

Extend `fillTableQuery()` by breaking a large table into batches using the `batchSize` parameter.    

```{r batch_fillAndWrite}
#' Utility Batch Fill and Write
#'
#' This is a utility function used in modSim to take a set of data, break it up
#'  into batches and write it to the required PostgreSQL database.
#'
#' @author Neil Kester, \email{nkester1@@jhu.edu}
#'
#' @param data A tibble of data
#' @param pgConnParam The connection strings required to connect to the PostgreSQL
#'  database of choice.
#' @param tableName The table of the PostgreSQL table this data is to be inserted
#'  into. It should include `\\"` around names requiring preservation of case.
#'  Optionally: It can include the SQL Field name specification to deal with a
#'  tibble that may not be ordered the same as the SQL Table. This should look
#'  like: `"\"tableName\" (\"fieldOneName\",\"fieldTwoName\")"`
#' @param batchSize How many records to execute at a time.
#' @param database What type of database is it going into? Options are: PostgreSQL
#'  or SQLite.
#'
#' @export batch_fillAndWrite
#'
#' @return Returns nothing
#'
#' @note Location: ./R/fct_utils_batchFillAndWrite.R
#' @note RMarkdown location: ./inst/step2_queryMongoAndFillPg/Step2_queryMongoAndFillPg.Rmd
batch_fillAndWrite <- function(data,
                               pgConnParam,
                               tableName,
                               batchSize=100,
                               database = "PostgreSQL"){
  
  if(database == "PostgreSQL"){
    
    serial <- "DEFAULT"
    
  }else if(database == "SQLite"){
    
    serial <- "NULL"
    
  }else{
    
    stop("The database parameter must be either \"SQLite\" or \"PostgreSQL\"")
    
  }
  
  
  startSize <- nrow(data)
  
  while(nrow(data) != 0){
    
    if(nrow(data)<batchSize){
      
      query_data <- fillTableQuery(data = data[1:nrow(data),],
                                   tableName = paste0("\"",
                                                      tableName,
                                                      "\" (",
                                                      paste0("\"",
                                                             names(data),
                                                             "\"",
                                                             collapse = ","),
                                                      ")"),
                                   serial = serial)
      
      sendPgFillTableQuery(query = query_data,
                           host = pgConnParam[["pgHost"]],
                           port = pgConnParam[["pgPort"]],
                           user = pgConnParam[["pgUser"]],
                           password = pgConnParam[["pgPass"]],
                           dbname = pgConnParam[["pgDb"]])
      
      data <- data[-(1:nrow(data)),]
      
      rm(query_data)
      
    }else{
      
      query_data <- fillTableQuery(data = data[1:batchSize,],
                                   tableName = paste0("\"",
                                                      tableName,
                                                      "\" (",
                                                      paste0("\"",
                                                             names(data),
                                                             "\"",
                                                             collapse = ","),
                                                      ")"),
                                   serial = serial)
      
      sendPgFillTableQuery(query = query_data,
                           host = pgConnParam[["pgHost"]],
                           port = pgConnParam[["pgPort"]],
                           user = pgConnParam[["pgUser"]],
                           password = pgConnParam[["pgPass"]],
                           dbname = pgConnParam[["pgDb"]])
      
      data <- data[-(1:batchSize),]
      
      rm(query_data)
      
    } # close else
    
    message(paste0((nrow(data)/startSize)*100,"% complete with ",tableName," table!"))
    
  }
  
  
} # batch_fillAndWrite

```

## Send a PostgreSQL Query: `sendPgFillTableQuery()`  

This utility function helps because it makes a connection to a specified PostgreSQL database, sends the provided query, and then disconnects. The re-use of this workflow warrants its encapsulation as a function.  

```{r utils_sendPgFillTableQuery}
#' Connect and Send Query to PostgreSQL
#'
#' This is a utility function to help connect to, send, and disconnect from
#'   a PostgreSQL database.
#'
#' @author Neil Kester, \email{nkester1@@jhu.edu}
#'
#' @param query This is a character string describing a query to send to PostgreSQL.
#'   Note that this conducts a `dbSendQuery` so this is not suitable for returning
#'   data but rather to write data or send CREATE instructions.
#' @param host A character vector to the desired PostgreSQL instance.
#' @param port An integer port number the PostreSQL instance listen on. Standard
#'   is 5432.
#' @param user A character vector of the user name you will connect as.
#' @param password A character vector of the user name's password in plain text.
#' @param dbname A character vector of the database name to connect to. If an
#'   empty string is provided it should connect to the admin db.
#'
#' @return Nothing
#'
#' @export sendPgFillTableQuery
#'
#' @importFrom DBI dbConnect dbSendQuery dbDisconnect
#' @importFrom RPostgreSQL PostgreSQL
#'
#' @note Location: ./R/fct_utils_sendPgFillTableQuery.R
#' @note RMarkdown location: ./inst/step2_queryMongoAndFillPg/Step2_queryMongoAndFillPg.Rmd
sendPgFillTableQuery <- function(query,
                                 host,
                                 port,
                                 user,
                                 password,
                                 dbname){

  pgConn <- DBI::dbConnect(drv = RPostgreSQL::PostgreSQL(),
                           host = host,
                           port = port,
                           user = user,
                           password = password,
                           dbname = dbname)

  DBI::dbSendQuery(conn = pgConn,
                           statement = query)

  DBI::dbDisconnect(conn = pgConn)

} # close sendPgFillTableQuery function
```


# Low Functions  

## Map Sensors and Entities: `mapSensorsAndEntities()`  

```{r fct_low_mapSensorsAndEntities}
#' Map Sensors and Entities
#'
#' This function accepts the same inputs as the basic MongoDB connection information
#'  and submits aggregation pipelines to MongoDB to return information about the
#'  sensors and entities present in the specified designPoint. 
#'
#' @author Neil Kester, \email{nkester1@@jhu.edu}
#'
#' @param mongoUri This is a double quoted character string of the connection
#'  object required for the simulation's MongoDB.
#' @param mongoDb This is a double quoted character string of the database name
#'  containing the required files.
#' @param mongoCollection This is a double quoted character string of the mongo
#'  collection within the named database containing the required information.
#' @param designPoint This is a single character string with the designPoint you
#'   would like to extract from the MongoDB and place into the PostgreSQL database.
#'   If multiple designPoints are required then execute this function multiple
#'   times. Note that this pulls ALL iterations executed for that designPoint.
#'
#' @return This function returns a four element named list of tibbles that
#'  describe all sensors and entities, their IDs and description.
#'
#' @importFrom tibble tibble
#' @importFrom mongolite mongo
#'
#' @note Location: ./R/fct_step2_low_mapSensorsAndEntities.R
#' @note RMarkdown location: ./inst/step2_queryMongoAndFillPg/Step2_queryMongoAndFillPg.Rmd
mapSensorsAndEntities <- function(mongoUri,
                                  mongoDb,
                                  mongoCollection,
                                  designPoint){

  mongoConnection <- mongolite::mongo(url = mongoUri,
                                      db = mongoDb,
                                      collection = mongoCollection)

  { # SENSOR DESCRIPTION ----
    
    pipeline_sensorDescription <- sprintf("[{\"$match\":{\"designPoint\": \"%s\"}},{\"$group\":{\"_id\": {\"sensorId\": \"$state.sensorId\",\"acquireSensorType\": \"$state.acquireSensorType\",\"magnification\": \"$state.magnification\"}}}]",
                                          designPoint)
    
    sensorDescription <- mongoConnection$aggregate(pipeline = pipeline_sensorDescription)
    names(sensorDescription) <- "id"
    sensorDescription <- tibble::tibble(sensorDescription$id)

  } # close SENSOR DESCRIPTION section

  { # ENTITY ID TO NAME ----
    
    pipeline_entityIdToName <- sprintf("[{\"$match\":{\"designPoint\": \"%s\"}},{\"$group\":{\"_id\": {\"entityId\": \"$state.entityId\",\"source\": \"$state.status.source\"}}}]",
                                          designPoint)
    
    entityIdToName <- mongoConnection$aggregate(pipeline = pipeline_entityIdToName)
    names(entityIdToName) <- "id"
    entityIdToName <- tibble::tibble(entityIdToName$id)

  } # close ENTITY ID TO NAME section

  { # SENSOR TO ENTITY ----
    
    pipeline_sensorToEntity <- sprintf("[{\"$match\":{\"designPoint\": \"%s\"}},{\"$group\":{\"_id\": {\"entityId\": \"$state.entityId\",\"sensorId\": \"$state.sensorId\"}}}]",
                                          designPoint)
    
    sensorToEntity <- mongoConnection$aggregate(pipeline = pipeline_sensorToEntity)
    names(sensorToEntity) <- "id"
    sensorToEntity <- tibble::tibble(sensorToEntity$id)

  } # close SENSOR TO ENTITY section

  { # META DATA ----
    
    pipeline_metaData <- sprintf("[{\"$match\":{\"designPoint\": \"%s\"}},{\"$group\":{\"_id\": {\"runId\": \"$runId\",\"designPoint\": \"$designPoint\",\"iteration\": \"$iteration\"}}}]",
                                          designPoint)
    
    metaData <- mongoConnection$aggregate(pipeline = pipeline_metaData)
    names(metaData) <- "id"
    metaData <- tibble::tibble(metaData$id)

  } # close META DATA section

  return(list("SensorDescription" = sensorDescription,
              "EntityIdToName" = entityIdToName,
              "SensorToEntityId" = sensorToEntity,
              "metaData" = metaData))

} # close mapSensorsAndEntities function

```

## Line of Sight Tables: `etlLosData()`  

This table describes when a sensor target has Line of Sight (LOS). This does not mean they have acquired each other but rather that their line of sight is not obstructed. 

```{r fct_low_etlLosData}

#' ETL Line of Sight Data
#'
#' This function queries the LOSTargetStatus collection in the simulation
#'  MongoDB, extracts the required fields, and then writes it to the PostgreSQL
#'  tables created by the `creatModSimDb` function. This operates as an iterator
#'  with MongoDB so it executes one record at a time.
#'
#' @author Neil Kester, \email{nkester1@@jhu.edu}
#'
#' @param mongoConnParam This is a two element named list including the "mongoUri"
#'   which includes the user name and password and a single character string and
#'   the "mongoDb" name as a character string.
#' @param pgConnParam A five element named list containing the following elements:
#'  "pgHost", "pgPort", "pgUser", "pgPass", and "pgDb".
#' @param designPoint This is a single character string with the designPoint you
#'   would like to extract from the MongoDB and place into the PostgreSQL database.
#'   If multiple designPoints are required then execute this function multiple
#'   times. Note that this pulls ALL iterations executed for that designPoint.
#'
#' @return This returns messages to the console updating the user on the function's
#'   status but returns no information.
#'
#' @importFrom mongolite mongo
#' @importFrom RPostgreSQL PostgreSQL
#' @importFrom DBI dbConnect dbSendQuery dbDisconnect
#' @importFrom tibble tibble
#' @importFrom utils txtProgressBar setTxtProgressBar
#'
#' @note Location: ./R/fct_step2_low_etlLosData.R
#' @note RMarkdown location: ./inst/step2_queryMongoAndFillPg/Step2_queryMongoAndFillPg.Rmd
etlLosData <- function(mongoConnParam,
                       pgConnParam,
                       designPoint){
  
  requireNamespace(package = "magrittr")
  
  { # Complete the MongoDB Connection Parameters ----
    
    mongoConnParam[["collection"]] <- "AcquireModel.event.LOSTargetStatus"
    
    mongoConnParam[["query"]] <- sprintf("{\"designPoint\": \"%s\"}",
                                         designPoint)
    
    mongoConnParam[["fields"]] <- "{\"_id\": true, \"runId\": true, \"runTime\": true,
  \"designPoint\": true, \"iteration\": true, \"time\": true,\"event\": true}"
    
  } # close Complete the MongoDB Connection Parameters
  
  { # Iterate
    
    message("Beginning Iteration")
    
    #> Connect to the MongoDB
    mongoConn <- mongolite::mongo(url = mongoConnParam$mongoUri,
                                  db = mongoConnParam$mongoDb,
                                  collection = mongoConnParam$collection)
    
    #> Return the number of records present in the query (for status)
    numRecs <- mongoConn$count(query = mongoConnParam$query)
    
    #> Create an iterator (cursor) in the MongoDB
    it <- mongoConn$iterate(query = mongoConnParam$query,
                            fields = mongoConnParam$fields)
    
    #> Connect to the PostgreSQL Database
    pgConn <- DBI::dbConnect(drv = RPostgreSQL::PostgreSQL(),
                             host = pgConnParam$pgHost,
                             port = pgConnParam$pgPort,
                             user = pgConnParam$pgUser,
                             password = pgConnParam$pgPass,
                             dbname = pgConnParam$pgDb)
    
    rdx <- list(1)
    
    #> Establish a progress bar
    pb <- utils::txtProgressBar(min = 0,
                                max = numRecs,
                                style = 3)
    
    #> The iterator returns `null` when it reaches the last record.
    while(!is.null(x <- it$one())){
      
      utils::setTxtProgressBar(pb = pb,
                               value = rdx[[1]])
      
      if(!is.null(x)){
        
        temp <- tibble::tibble("losState_pkid" = NA,
                               "id" = x$`_id`,
                               "runId" = x$runId,
                               "runTime" = x$runTime,
                               "designPoint" = x$designPoint,
                               "iteration" = x$iteration,
                               "time_ms" = x$time,
                               "time_s" = x$time/1000,
                               "sensorId" = x$event$sensorId,
                               "targetId" = x$event$targetId,
                               "hasLOS" = x$event$hasLOS) 
        
        temp_query <- tryCatch(expr = fillTableQuery(data = temp,
                                                     tableName = "\"losState\"",
                                                     serial = "DEFAULT"),
                               warning = function(w){
                                 message(w)
                                 return("")
                               },
                               error = function(e){
                                 message(e)
                                 return("")
                               })
        
        DBI::dbSendQuery(conn = pgConn,
                         statement = temp_query)
        
        rdx[[1]] <- rdx[[1]] + 1
        
      } # close if is not null
      
    } # close While loop for iterator
    
    #> Clean up the progress bar object
    close(pb)
    rm(pb)
    
    #> Disconnect from the databases when the job is complete.
    DBI::dbDisconnect(conn = pgConn)
    mongoConn$disconnect()
    
  } # close Iterate
  
  message("LOS Complete")
  
} # close fct_low_etlLosData


```

## Sensor Acquisition State Tables: `etlSensorAcq()`  

Unlike the LOS table, this table shows the state of acquisition between each sensor target pair. It also shows the previous acquisition state so we can determine when the acquistion level changes. 

```{r fct_low_etlSensorAcq}

#' ETL Sensor Acquisition Data
#'
#' This function queries the C2SimulationMessage collection in the simulation
#'  MongoDB, extracts the required fields, and then writes it to the PostgreSQL
#'  tables created by the `creatModSimDb` function. This operates as an iterator
#'  with MongoDB so it executes one record at a time.
#'
#' @author Neil Kester, \email{nkester1@@jhu.edu}
#'
#' @param mongoConnParam This is a two element named list including the "mongoUri"
#'   which includes the user name and password and a single character string and
#'   the "mongoDb" name as a character string.
#' @param pgConnParam A five element named list containing the following elements:
#'  "pgHost", "pgPort", "pgUser", "pgPass", and "pgDb".
#' @param designPoint This is a single character string with the designPoint you
#'   would like to extract from the MongoDB and place into the PostgreSQL database.
#'   If multiple designPoints are required then execute this function multiple
#'   times. Note that this pulls ALL iterations executed for that designPoint.
#'
#' @return This returns messages to the console updating the user on the function's
#'   status but returns no information.
#'
#' @importFrom mongolite mongo
#' @importFrom RPostgreSQL PostgreSQL
#' @importFrom DBI dbConnect dbSendQuery dbDisconnect
#' @importFrom tibble tibble
#' @importFrom utils txtProgressBar setTxtProgressBar
#'
#' @note Location: ./R/fct_step2_low_etlSensorAcq.R
#' @note RMarkdown location: ./inst/step2_queryMongoAndFillPg/Step2_queryMongoAndFillPg.Rmd
etlSensorAcq <- function(mongoConnParam,
                         pgConnParam,
                         designPoint){

  { # Complete the MongoDB Connection Parameters ----
    
    mongoConnParam[["collection"]] <- "AcquireModel.event.C2SimulationMessage"
    
    mongoConnParam[["query"]] <- sprintf("{\"designPoint\": \"%s\", \"event.messageData.javaClass\": \"sensorproto.SensorModel$DetectedTarget\"}",
                                         designPoint)
    
    mongoConnParam[["fields"]] <- "{\"runId\": 1,\"runTime\": 1, \"designPoint\": 1,\"iteration\": 1, \"time\": 1, \"event.receiverId\": 1, \"event.senderId\": 1,\"event.messageData.any.sensorDetection\": 1}"
    
  } # close Complete the MongoDB Connection Parameters
  
  { # Iterate
    
    message("Beginning Iteration")
    
    #> Connect to the MongoDB
    mongoConn <- mongolite::mongo(url = mongoConnParam$mongoUri,
                                  db = mongoConnParam$mongoDb,
                                  collection = mongoConnParam$collection)
    
    #> Return the number of records present in the query (for status)
    numRecs <- mongoConn$count(query = mongoConnParam$query)
    
    #> Create an iterator (cursor) in the MongoDB    
    it <- mongoConn$iterate(query = mongoConnParam$query,
                            fields = mongoConnParam$fields)
    
    #> Connect to the PostgreSQL Database
    pgConn <- DBI::dbConnect(drv = RPostgreSQL::PostgreSQL(),
                             host = pgConnParam$pgHost,
                             port = pgConnParam$pgPort,
                             user = pgConnParam$pgUser,
                             password = pgConnParam$pgPass,
                             dbname = pgConnParam$pgDb)
    
    rdx <- list(1)
    
    #> Establish a progress bar
    pb <- utils::txtProgressBar(min = 0,
                                max = numRecs,
                                style = 3)
    
    #> The iterator returns `null` when it reaches the last record.    
    while(!is.null(x <- it$one())){
      
      utils::setTxtProgressBar(pb = pb,
                               value = rdx[[1]])
      
      if(!is.null(x)){
        
        temp <- tibble::tibble("sensorAcqState_pkid" = NA,
                               "id" = x$`_id`,
                               "runId" = x$runId,
                               "runTime" = x$runTime,
                               "designPoint" = x$designPoint,
                               "iteration" = x$iteration,
                               "time_ms" = x$time,
                               "time_s" = x$time/1000,
                               "receiverId" = x$event$receiverId,
                               "senderId" = x$event$senderId,
                               "sensorId" = x$event$messageData$any$sensorDetection$sensorId,
                               "entityId" = x$event$messageData$any$sensorDetection$entityId,
                               "targetId" = x$event$messageData$any$sensorDetection$targetId,
                               "detectionLevel" = x$event$messageData$any$sensorDetection$detectionLevel,
                               "previousDetectionLevel" = x$event$messageData$any$sensorDetection$previousDetectionLevel,
                               "timeToDetection" = x$event$messageData$any$sensorDetection$timeToDetection) 
        
        temp_query <- tryCatch(fillTableQuery(data = temp,
                                              tableName = "\"sensorAcqState\"",
                                              serial = "DEFAULT"),
                               warning = function(w){
                                 message(w)
                                 return("")
                               },
                               error = function(e){
                                 message(e)
                                 return("")
                               })
        
        DBI::dbSendQuery(conn = pgConn,
                         statement = temp_query)
        
      }
      
      rdx[[1]] <- rdx[[1]] + 1
      
    } # close if is not null
    
    #> Clean up the progress bar object
    close(pb)
    rm(pb)
    
    #> Disconnect from the database when the job is complete.
    DBI::dbDisconnect(conn = pgConn)
    mongoConn$disconnect()
    
  } # close Iterate

  message("Sensor Acq ETL Complete")
  
} # close fct_low_etlSensorAcq

```

# Mid Function

## Sensor to Entity Mapping: `etlSensorToEntityMappingTables()`  

Select information from the `AcquireModel.state.sensors` MongoDB collection and write it to the `sensorDescription`, `entityIdToName`, and `sensorToEntityId` PostgreSQL tables.  

These tables provide maps to the various entity and sensor Ids produced by the simulation.  

```{r fct_mid_etlSensorToEntityMappingTables}

#' ETL Sensor To Entity Mapping Tables
#'
#' This function queries the simulation's MongoDB, extracting specific information
#'   from the state.sensors collection, and it writes the resulting data to tables 
#'   in PostgreSQL created by the `createModSimDb` function.
#'
#' @author Neil Kester, \email{nkester1@@jhu.edu}
#'
#' @param mongoConnParam This is a two element named list including the "mongoUri"
#'   which includes the user name and password and a single character string and
#'   the "mongoDb" name as a character string.
#' @param pgConnParam A five element named list containing the following elements:
#'  "pgHost", "pgPort", "pgUser", "pgPass", and "pgDb".
#' @param designPoint This is a single character string with the designPoint you
#'   would like to extract from the MongoDB and place into the PostgreSQL database.
#'   If multiple designPoints are required then execute this function multiple
#'   times. Note that this pulls ALL iterations executed for that designPoint.
#' @param batchSize A numeric integer representing how many records you want to
#'  write to the PostgreSQL database at a time.
#'
#' @return This returns messages to the console updating the user on the function's
#'   status but returns no information.
#'
#' @importFrom dplyr distinct mutate case_when rename
#' @importFrom stringr str_detect str_extract
#'
#' @note Location: ./R/fct_step2_mid_etlSensorToEntityMappingTables.R
#' @note RMarkdown location: ./inst/step2_queryMongoAndFillPg/Step2_queryMongoAndFillPg.Rmd
etlSensorToEntityMappingTables <- function(mongoConnParam,
                                           pgConnParam,
                                           designPoint,
                                           batchSize){
  
  requireNamespace(package = "magrittr")
  
  { # Complete the MongoDB Connection Parameters ----
    
    mongoConnParam[["collection"]] <- "AcquireModel.state.sensors"
    
  } # close Complete the MongoDB Connection Parameters
  
  { # Extract ----
    
    { # Query MongoDb and unnest information about sensors and entities ----
      
      message("Extracting data from MongoDB")
      
      entitySensorMapping <- mapSensorsAndEntities(mongoUri = mongoConnParam[["mongoUri"]],
                                                   mongoDb = mongoConnParam[["mongoDb"]],
                                                   mongoCollection = mongoConnParam[["collection"]],
                                                   designPoint = designPoint)
      
      metaData <- entitySensorMapping$metaData
      
    } # close Query MongoDb and unnest information about sensors and entities section
    
  } # close Extract section
  
  { # Transform and Load ----
    
    { # sensorDescription ----
      
      message("Transforming and loading sensorDescription data.")
      
      entitySensorMapping$SensorDescription <- entitySensorMapping$SensorDescription %>%
        dplyr::mutate(.data = .,
                      designPoint = dplyr::distinct(metaData,
                                                    designPoint)[[1]],
                      sensorId_pkId = NA)
      
      batch_fillAndWrite(data = entitySensorMapping$SensorDescription,
                         pgConnParam = pgConnParam,
                         tableName = "sensorDescription",
                         batchSize = batchSize,
                         database = "PostgreSQL")
      
    } # close sensorDescription section
    
    { # entityIdToName ----
      
      message("Transforming and loading entityIdToName data.")
      
      entitySensorMapping$EntityIdToName <- entitySensorMapping$EntityIdToName %>%
        dplyr::mutate(.data = .,
                      designPoint = dplyr::distinct(metaData,
                                                    designPoint)[[1]],
                      force = dplyr::case_when(
                        stringr::str_detect(string = source,
                                            pattern = "^(?i)(blueforce)") ~ "BLUEFORCE",
                        stringr::str_detect(string = source,
                                            pattern = "^(?i)(redforce)") ~ "REDFORCE",
                        TRUE ~ "OTHER"
                      ),
                      shortName = stringr::str_extract(string = source,
                                                       pattern = "[^/]*$"),
                      entityId_pkId = NA)
      
      batch_fillAndWrite(data = entitySensorMapping$EntityIdToName,
                         pgConnParam = pgConnParam,
                         tableName = "entityIdToName",
                         batchSize = batchSize,
                         database = "PostgreSQL")
      
      
    } # close entityIdToName section
    
    { # sensorToEntityId ----
      
      message("Transforming and loading sensorToEntityId data.")
      
      entitySensorMapping$SensorToEntityId <- entitySensorMapping$SensorToEntityId %>%
        dplyr::mutate(.data = .,
                      designPoint = dplyr::distinct(metaData,
                                                    designPoint)[[1]],
                      sensorToEntityId_pkId = NA)
      
      batch_fillAndWrite(data = entitySensorMapping$SensorToEntityId,
                         pgConnParam = pgConnParam,
                         tableName = "sensorToEntityId",
                         batchSize = batchSize,
                         database = "PostgreSQL")
      
    } # close sensorToEntityId section
    
  } # close Transform and Load section
  
} # close fct_step2_mid_etlSensorToEntityMappingTables


```

# High Function  

## Query MongoDB and Fill PostgreSQL: `queryMongoAndFillPg()`  

This function takes three elements as inputs, `mongoConnParam`, `pgConnParam`, and `designPoint` which are used by each of the low functions. Each low function queries and unpackes different collections from the Simulation MongoDB and stores them in the PostgreSQL Relational Database created in the previous step.  

This is the only function exported from this step in the process.  

```{r fct_high_queryMongoAndFillPg}
#' Query MongoDB And Fill PostgreSQL
#'
#' This is the high level function executed in step 2 to extract data from the
#'   simulation's No-SQL MongoDB, transform it, and load it into the analytic
#'   PostgreSQL relational database created with `createModSimDb`. This is
#'   executed for one design point at a time.
#'
#' @author Neil Kester, \email{nkester1@@jhu.edu}
#'
#' @param mongoConnParam This is a two element named list including the "mongoUri"
#'   which includes the user name and password and a single character string and
#'   the "mongoDb" name as a character string.
#' @param pgConnParam A five element named list containing the following elements:
#'  "pgHost", "pgPort", "pgUser", "pgPass", and "pgDb".
#' @param designPoint This is a single character string with the designPoint you
#'   would like to extract from the MongoDB and place into the PostgreSQL database.
#'   If multiple designPoints are required then execute this function multiple
#'   times. Note that this pulls ALL iterations executed for that designPoint.
#' @param batchSize A numeric integer representing how many records you want to
#'  write to the PostgreSQL database at a time.
#' @param Materialized This is a boolean response if you want to make the view
#'  materialized or not. Materialized views greatly improve the performance of 
#'  queries although they duplicate data in memory so they can be costly to 
#'  disk space. Default is FALSE.
#'
#' @return This returns messages to the console updating the user on the function's
#'   status but returns no information.
#'
#' @export Step2_queryMongoAndFillPg
#'
#' @note Location: ./R/fct_step2_high_queryMongoAndFillPg.R
#' @note RMarkdown location: ./inst/step2_queryMongoAndFillPg/Step2_queryMongoAndFillPg.Rmd
Step2_queryMongoAndFillPg <- function(mongoConnParam,
                                pgConnParam,
                                designPoint,
                                batchSize=100,
                                Materialized = FALSE){
  
  message("Reading from MongDB Acquire.State.Sensor collection and writing to PostgreSQL sensorDescription, entityIdToName, sensorToEntityId, and unnestedSensorState tables.")
  
  etlSensorToEntityMappingTables(mongoConnParam = mongoConnParam,
                                 pgConnParam = pgConnParam,
                                 designPoint = designPoint,
                                 batchSize = batchSize)
  
  message("Reading from MongoDB AcquireModel.event.LOSTargetStatus collection and writing to PostgreSQL losState table.")
  
  etlLosData(mongoConnParam = mongoConnParam,
             pgConnParam = pgConnParam,
             designPoint = designPoint)
  
  message("Reading from MongoDb AcquireModel.event.C2SimulationMessage collection and writing to PostgreSQL sensorAcqState table.")
  
  etlSensorAcq(mongoConnParam = mongoConnParam,
               pgConnParam = pgConnParam,
               designPoint = designPoint)
  
  { # Refresh the materialized Views ----
    
    if(Materialized){
      
      #> After writing all of the data for this designPoint to the PostgreSQL
      #>  database, update the materialized views so that the data is ready to be
      #>  queried. Failing to do this negates the utility of having a pre-executed
      #>  view ready to be queried.
      
      pgConn <- DBI::dbConnect(drv = RPostgreSQL::PostgreSQL(),
                               host = pgConnParam[["pgHost"]],
                               port = pgConnParam[["pgPort"]],
                               user = pgConnParam[["pgUser"]],
                               password = pgConnParam[["pgPass"]],
                               dbname = pgConnParam[["pgDb"]])
      
      DBI::dbSendQuery(conn = pgConn,
                       statement = "REFRESH MATERIALIZED VIEW los_sensor_target_pairs_materialized")
      
      DBI::dbSendQuery(conn = pgConn,
                       statement = "REFRESH MATERIALIZED VIEW acq_sensor_target_pairs_materialized")
      
      DBI::dbDisconnect(conn = pgConn)
      
      rm(pgConn)
      
    } # close if materialized
    
  } # close Refresh the materialized Views section
  
  return("Complete!")
  
} # close Step2_queryMongoAndFillPg

```   

# Execution

This is how this function would be executed . Note, the following is the structure of the input parameters.  

mongoConnParam:  

```{r,echo=FALSE}
list("mongoUri" = "monogdb://<username>:<password>@<host.path.to.mongodb>:<port>",
     "mongoDb" = "databaseName")
```  

pgConnParam:  

```{r, echo=FALSE}
list("pgHost" = "<host.path.to.postgreSQL>",
     "pgPort" = 5432,
     "pgUser" = "<username>",
     "pgPass" = "<password>",
     "pgDb" = "<databaseName>")
```

Implement the function in this way:  

```{r, eval=FALSE}

#> These source files read in the mongoURI and mongoDb elements and the PostgreSQL connection objects.

source("../connectionObjects/pgConnectionObj.R")
source("../connectionObjects/mongoConnectionObj.R")

#> Add to the pgConnParam the database name 

pgConnParam[["pgDb"]] <- "modSimIt5"  

#> The design point I want to load  

designPoint <- "myDesignPoint"

#> Execute the function  

response <- queryMongoAndFillPg(mongoConnParam = mongoConnParam,
                    pgConnParam = pgConnParam,
                    designPoint = designPoint,
                    Materialized = FALSE)


```


